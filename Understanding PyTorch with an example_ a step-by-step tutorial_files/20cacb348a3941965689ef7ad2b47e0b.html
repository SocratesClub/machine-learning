<!DOCTYPE html>
<!-- saved from url=(0089)https://towardsdatascience.com/media/20cacb348a3941965689ef7ad2b47e0b?postId=81fc5f8c4e8e -->
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><title>torch101_imports.py – Medium</title><meta name="description" content="GitHub Gist: instantly share code, notes, and snippets."><meta name="twitter:widgets:csp" content="on"><meta name="robots" content="noindex"><!--<base target="_blank">--><base href="." target="_blank"><style>body {text-rendering: optimizeLegibility; -webkit-font-smoothing: antialiased; -moz-osx-font-smoothing: grayscale; font-family: "ff-tisa-web-pro", Georgia, Cambria, "Times New Roman", Times, serif; font-weight: 400; color: #333332; font-size: 18px; line-height: 1.4; margin: 0; background-color: white; overflow: hidden;}iframe {max-width: 100%;}</style></head><body><style>.gist .gist-file { margin-bottom: 0 !important; }.gist { text-rendering: auto; }</style><script src="./aa4be182657db392fecb3ee56ce5a941.js" charset="utf-8"></script><link rel="stylesheet" href="./gist-embed-a9a1cf2ca01efd362bfa52312712ae94.css"><div id="gist95928743" class="gist">
    <div class="gist-file">
      <div class="gist-data">
        <div class="js-gist-file-update-container js-task-list-container file-box">
  <div id="file-torch101_imports-py" class="file">
    

  <div itemprop="text" class="Box-body p-0 blob-wrapper data type-python ">
      
<table class="highlight tab-size js-file-line-container" data-tab-size="8">
      <tbody><tr>
        <td id="file-torch101_imports-py-L1" class="blob-num js-line-number" data-line-number="1"></td>
        <td id="file-torch101_imports-py-LC1" class="blob-code blob-code-inner js-file-line"><span class="pl-k">import</span> torch</td>
      </tr>
      <tr>
        <td id="file-torch101_imports-py-L2" class="blob-num js-line-number" data-line-number="2"></td>
        <td id="file-torch101_imports-py-LC2" class="blob-code blob-code-inner js-file-line"><span class="pl-k">import</span> torch.optim <span class="pl-k">as</span> optim</td>
      </tr>
      <tr>
        <td id="file-torch101_imports-py-L3" class="blob-num js-line-number" data-line-number="3"></td>
        <td id="file-torch101_imports-py-LC3" class="blob-code blob-code-inner js-file-line"><span class="pl-k">import</span> torch.nn <span class="pl-k">as</span> nn</td>
      </tr>
      <tr>
        <td id="file-torch101_imports-py-L4" class="blob-num js-line-number" data-line-number="4"></td>
        <td id="file-torch101_imports-py-LC4" class="blob-code blob-code-inner js-file-line"><span class="pl-k">from</span> torchviz <span class="pl-k">import</span> make_dot</td>
      </tr>
      <tr>
        <td id="file-torch101_imports-py-L5" class="blob-num js-line-number" data-line-number="5"></td>
        <td id="file-torch101_imports-py-LC5" class="blob-code blob-code-inner js-file-line">
</td>
      </tr>
      <tr>
        <td id="file-torch101_imports-py-L6" class="blob-num js-line-number" data-line-number="6"></td>
        <td id="file-torch101_imports-py-LC6" class="blob-code blob-code-inner js-file-line">device <span class="pl-k">=</span> <span class="pl-s"><span class="pl-pds">'</span>cuda<span class="pl-pds">'</span></span> <span class="pl-k">if</span> torch.cuda.is_available() <span class="pl-k">else</span> <span class="pl-s"><span class="pl-pds">'</span>cpu<span class="pl-pds">'</span></span></td>
      </tr>
      <tr>
        <td id="file-torch101_imports-py-L7" class="blob-num js-line-number" data-line-number="7"></td>
        <td id="file-torch101_imports-py-LC7" class="blob-code blob-code-inner js-file-line">
</td>
      </tr>
      <tr>
        <td id="file-torch101_imports-py-L8" class="blob-num js-line-number" data-line-number="8"></td>
        <td id="file-torch101_imports-py-LC8" class="blob-code blob-code-inner js-file-line"><span class="pl-c"><span class="pl-c">#</span> Our data was in Numpy arrays, but we need to transform them into PyTorch's Tensors</span></td>
      </tr>
      <tr>
        <td id="file-torch101_imports-py-L9" class="blob-num js-line-number" data-line-number="9"></td>
        <td id="file-torch101_imports-py-LC9" class="blob-code blob-code-inner js-file-line"><span class="pl-c"><span class="pl-c">#</span> and then we send them to the chosen device</span></td>
      </tr>
      <tr>
        <td id="file-torch101_imports-py-L10" class="blob-num js-line-number" data-line-number="10"></td>
        <td id="file-torch101_imports-py-LC10" class="blob-code blob-code-inner js-file-line">x_train_tensor <span class="pl-k">=</span> torch.from_numpy(x_train).float().to(device)</td>
      </tr>
      <tr>
        <td id="file-torch101_imports-py-L11" class="blob-num js-line-number" data-line-number="11"></td>
        <td id="file-torch101_imports-py-LC11" class="blob-code blob-code-inner js-file-line">y_train_tensor <span class="pl-k">=</span> torch.from_numpy(y_train).float().to(device)</td>
      </tr>
      <tr>
        <td id="file-torch101_imports-py-L12" class="blob-num js-line-number" data-line-number="12"></td>
        <td id="file-torch101_imports-py-LC12" class="blob-code blob-code-inner js-file-line">
</td>
      </tr>
      <tr>
        <td id="file-torch101_imports-py-L13" class="blob-num js-line-number" data-line-number="13"></td>
        <td id="file-torch101_imports-py-LC13" class="blob-code blob-code-inner js-file-line"><span class="pl-c"><span class="pl-c">#</span> Here we can see the difference - notice that .type() is more useful</span></td>
      </tr>
      <tr>
        <td id="file-torch101_imports-py-L14" class="blob-num js-line-number" data-line-number="14"></td>
        <td id="file-torch101_imports-py-LC14" class="blob-code blob-code-inner js-file-line"><span class="pl-c"><span class="pl-c">#</span> since it also tells us WHERE the tensor is (device)</span></td>
      </tr>
      <tr>
        <td id="file-torch101_imports-py-L15" class="blob-num js-line-number" data-line-number="15"></td>
        <td id="file-torch101_imports-py-LC15" class="blob-code blob-code-inner js-file-line"><span class="pl-c1">print</span>(<span class="pl-c1">type</span>(x_train), <span class="pl-c1">type</span>(x_train_tensor), x_train_tensor.type())</td>
      </tr>
</tbody></table>


  </div>

  </div>
</div>

      </div>
      <div class="gist-meta">
        <a href="https://gist.github.com/dvgodoy/aa4be182657db392fecb3ee56ce5a941/raw/a517b17140ea8e74a2a2365a519693f16bd23c43/torch101_imports.py" style="float:right">view raw</a>
        <a href="https://gist.github.com/dvgodoy/aa4be182657db392fecb3ee56ce5a941#file-torch101_imports-py">torch101_imports.py</a>
        hosted with ❤ by <a href="https://github.com/">GitHub</a>
      </div>
    </div>
</div>
<script>var height = -1; var delayMs = 200;function notifyResize(height) {height = height ? height : document.documentElement.offsetHeight; var resized = false; if (window.donkey && donkey.resize) {donkey.resize(height); resized = true;}if (parent && parent._resizeIframe) {var obj = {iframe: window.frameElement, height: height}; parent._resizeIframe(obj); resized = true;}if (window.location && window.location.hash === "#amp=1" && window.parent && window.parent.postMessage) {window.parent.postMessage({sentinel: "amp", type: "embed-size", height: height}, "*");}if (window.webkit && window.webkit.messageHandlers && window.webkit.messageHandlers.resize) {window.webkit.messageHandlers.resize.postMessage(height); resized = true;}return resized;}function maybeResize() {if (document.documentElement.offsetHeight != height && notifyResize()) {height = document.documentElement.offsetHeight;}delayMs = Math.min(delayMs * 2, 1000000); setTimeout(maybeResize, delayMs);}maybeResize();</script></body></html>